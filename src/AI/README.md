# Oasis AI Service - GPU Optimized

AI service for Oasis with GPU-optimized local models and function calling support.

## ğŸš€ Features

- **GPU Acceleration**: Optimized for NVIDIA GPUs with CUDA support
- **Local Models**: Run models locally with node-llama-cpp
- **Function Calling**: Built-in function calling capabilities
- **Performance Monitoring**: GPU diagnostics and benchmarking tools
- **GPU Optimized**: Specifically optimized for high-end GPUs

## ğŸ“‹ Requirements

- Node.js 18+ with ES modules support
- NVIDIA GPU with driver 450.80.02+ (RTX series recommended)
- 8GB+ VRAM (24GB recommended for GPU)
- Windows 10/11 or Linux

## ğŸ› ï¸ Installation

```bash
npm install
npm run setup
```

## ğŸ¯ Available Scripts

### Core Services
- `npm start` - Start the AI service
- `npm run build` - Build the service
- `npm run setup` - Run installation setup

### GPU Diagnostics & Testing
- `npm run gpu:check` - **Check GPU status and get recommendations**
- `npm run diagnostic` - Same as gpu:check (alias)
- `npm run verify-gpu` - Full GPU verification (check + simple test)

### GPU Performance Tests
- `npm run gpu:test-simple` - Simple GPU performance test
- `npm run gpu:test-real` - Real model test with VRAM monitoring
- `npm run test:local-model` - Test local model loading (alias)

### Function Calling Tests
- `npm run gpu:test-functions` - Test functions with custom wrapper
- `npm run gpu:test-simple-functions` - Test functions without wrapper (recommended)
- `npm run test:functions` - Simple function tests (alias)

### Legacy Llama Tests
- `npm run llama:test-handler` - Test llama handler
- `npm run llama:test-functions` - Test llama functions
- `npm run llama:test-no-functions` - Test without functions

### Comprehensive Testing
- `npm run test:all-gpu` - Run all GPU tests (check + simple + functions)
- `npm run benchmark` - Performance benchmark (simple + functions)

## ğŸ® Quick Start

1. **Check your GPU setup:**
   ```bash
   npm run gpu:check
   ```

2. **Run a simple test:**
   ```bash
   npm run gpu:test-simple
   ```

3. **Test with functions:**
   ```bash
   npm run test:functions
   ```

4. **Run comprehensive test:**
   ```bash
   npm run test:all-gpu
   ```

## ğŸ”§ GPU Configuration

The service is optimized for GPU with these settings:

```javascript
const config = {
  gpu: true,              // GPU enabled by default
  gpuLayers: undefined,   // Auto - load maximum layers on GPU
  vramPadding: 256,       // 256MB padding for 24GB VRAM
};
```

For different GPUs, adjust `vramPadding`:
- **4GB VRAM**: 64MB padding
- **8GB VRAM**: 128MB padding  
- **16GB+ VRAM**: 256MB+ padding

## ğŸ“Š Performance Metrics

Expected performance on GPU:
- **Model Loading**: ~5-6GB VRAM usage
- **Inference Speed**: 200-500ms per response
- **Tokens/Second**: 15-30 (depending on model size)

## ğŸ› Troubleshooting

### GPU Not Detected
```bash
npm run gpu:check
```
Check output for recommendations.

### Model Loading Issues
Ensure model file exists at:
```
src/AI/models/oasis-42-1-chat.Q4_K_M.gguf
```

### Function Calling Problems
Use simple functions test:
```bash
npm run test:functions
```

### Performance Issues
Run benchmark:
```bash
npm run benchmark
```

## ğŸ“ File Structure

```
src/AI/
â”œâ”€â”€ package.json
â”œâ”€â”€ ai_service.mjs
â”œâ”€â”€ models/
â”‚   â””â”€â”€ oasis-42-1-chat.Q4_K_M.gguf
â””â”€â”€ plugins/
    â”œâ”€â”€ node_llama_cpp_handler.mjs     # Main model handler
    â”œâ”€â”€ llama_functions_handler.mjs   # Function calling handler
    â”œâ”€â”€ gpu_diagnostics.mjs         # GPU diagnostic tools
    â”œâ”€â”€ test_gpu_simple.mjs         # Simple GPU test
    â”œâ”€â”€ test_simple_functions.mjs   # Function test
    â””â”€â”€ test_*.mjs                  # Various test files
```

## ğŸ† GPU Optimization Results

âœ… **GPU enabled and working**
âœ… **5+ GB VRAM utilized** (model fully loaded on GPU)
âœ… **Sub-second responses** (excellent performance)
âœ… **GPU fully utilized** (maximum acceleration)

Your node-llama-cpp client is now GPU-optimized! ğŸš€