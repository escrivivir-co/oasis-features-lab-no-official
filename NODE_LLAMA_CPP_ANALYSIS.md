# =============================================================================
# NODE-LLAMA-CPP - An√°lisis de Capacidades y Requisitos
# Versi√≥n: 3.13.0 (usada en OASIS)
# =============================================================================

## üéØ **CAPACIDADES DE ACELERACI√ìN GPU**

### **1. NO necesita CUDA instalado directamente**
- `node-llama-cpp` compila su propia versi√≥n de llama.cpp
- Incluye soporte CUDA integrado si detecta GPU NVIDIA
- **NO requiere CUDA SDK/Toolkit instalado por el usuario**
- Descarga autom√°ticamente las librer√≠as CUDA necesarias

### **2. Backends de GPU soportados:**

#### **NVIDIA (CUDA) ‚úÖ Recomendado**
- **Autom√°tico:** Detecta y usa GPU NVIDIA sin configuraci√≥n
- **Requisitos:** Solo drivers NVIDIA actualizados
- **Performance:** 10-50x m√°s r√°pido que CPU
- **VRAM:** Funciona desde 4GB, √≥ptimo 8GB+

#### **AMD (ROCm) ‚ö†Ô∏è Experimental**
- **Soporte:** Limitado en node-llama-cpp
- **Requisitos:** ROCm drivers + configuraci√≥n manual
- **Performance:** Variable, 2-10x m√°s r√°pido que CPU

#### **Intel GPU (OpenCL) ‚ö†Ô∏è B√°sico**
- **Soporte:** Muy limitado
- **Requisitos:** Intel GPU drivers
- **Performance:** Marginal vs CPU

#### **Apple Silicon (Metal) ‚úÖ Nativo**
- **Soporte:** Excelente en M1/M2/M3
- **Autom√°tico:** Sin configuraci√≥n necesaria
- **Performance:** 5-20x m√°s r√°pido que CPU

### **3. Compilaci√≥n autom√°tica:**
```bash
# Lo que hace node-llama-cpp autom√°ticamente:
1. Detecta hardware disponible
2. Descarga llama.cpp source
3. Compila con opciones optimizadas
4. Integra CUDA si est√° disponible
5. Fallback a CPU si GPU no disponible
```

## ‚öôÔ∏è **CONFIGURACI√ìN EN OASIS**

### **Variables de entorno utilizadas:**
```bash
GPU_ENABLED=true/false     # Activar/desactivar GPU
GPU_LAYERS=auto           # Capas en GPU (auto, 0-100, n√∫mero espec√≠fico)
VRAM_PADDING=256          # MB de VRAM a reservar
```

### **Configuraci√≥n Docker actual:**
```yaml
# docker-compose.yml - NVIDIA GPU
deploy:
  resources:
    reservations:
      devices:
        - driver: nvidia
          count: all
          capabilities: [gpu]
```

## üîß **REQUISITOS DEL SISTEMA**

### **Para GPU NVIDIA (recomendado):**
- ‚úÖ **GPU:** GTX 1060+ / RTX series / Tesla / Quadro
- ‚úÖ **VRAM:** M√≠nimo 4GB, recomendado 8GB+
- ‚úÖ **Drivers:** NVIDIA 470.57.02+ (Linux) / 471.96+ (Windows)
- ‚úÖ **Docker:** NVIDIA Container Toolkit
- ‚ùå **NO necesita:** CUDA SDK instalado manualmente

### **Para CPU (fallback):**
- ‚úÖ **CPU:** x86_64 con AVX2 (Intel 2013+, AMD 2015+)
- ‚úÖ **RAM:** M√≠nimo 8GB, recomendado 16GB+
- ‚úÖ **Threads:** M√°s cores = mejor performance

### **Para desarrollo:**
- ‚úÖ **Compiladores:** gcc/clang, cmake, make
- ‚úÖ **Git:** Para descargar llama.cpp source
- ‚úÖ **Python:** Para scripts de build

## üìä **PERFORMANCE ESPERADA**

### **Modelo Q4_K_M (4GB):**
```
RTX 4090:     ~50-80 tokens/sec
RTX 3080:     ~30-50 tokens/sec  
GTX 1080 Ti:  ~15-25 tokens/sec
CPU (16 core): ~3-8 tokens/sec
CPU (8 core):  ~1-4 tokens/sec
```

### **Factores que afectan performance:**
- **VRAM disponible:** M√°s VRAM = m√°s capas en GPU
- **Modelo size:** Q4_K_M vs Q8_0 vs F16
- **Context length:** M√°s contexto = m√°s memoria
- **Batch size:** Para m√∫ltiples requests

## üöÄ **OPTIMIZACIONES APLICADAS EN OASIS**

### **1. Configuraci√≥n GPU autom√°tica:**
```javascript
// En ai_service_standalone.mjs
const GPU_ENABLED = process.env.GPU_ENABLED === 'true';
const GPU_LAYERS = process.env.GPU_LAYERS === 'auto' ? undefined : parseInt(process.env.GPU_LAYERS);
const VRAM_PADDING = parseInt(process.env.VRAM_PADDING) || 256;
```

### **2. Fallback inteligente:**
```javascript
llamaInstance = await getLlama({
  gpu: GPU_ENABLED,           // Auto-detect y usar GPU si disponible
  vramPadding: VRAM_PADDING,  // Reservar VRAM para estabilidad
  logger: GPU_ENABLED ? {     // Logging detallado para GPU
    log: (level, message) => console.log(`[Llama ${level}]`, message),
  } : undefined
});
```

### **3. Configuraci√≥n de contexto optimizada:**
```javascript
context = await model.createContext({
  threads: GPU_ENABLED ? 1 : 4,  // Menos threads con GPU
  contextSize: 4096,             // Tama√±o de contexto balanceado
});
```

## üéØ **RECOMENDACIONES PARA TU SETUP**

### **Si tienes NVIDIA GPU:**
```bash
npm run dev:gpu      # Usar configuraci√≥n GPU optimizada
npm run gpu:verify   # Verificar que funciona correctamente
```

### **Si no tienes GPU compatible:**
```bash
npm run dev:cpu      # Usar configuraci√≥n CPU optimizada
```

### **Para debugging:**
```bash
npm run gpu:detect   # Detectar hardware disponible
npm run gpu:logs     # Monitorear logs de GPU/AI
npm run ai:status    # Estado detallado del servicio
```

## ‚ö†Ô∏è **PROBLEMAS COMUNES Y SOLUCIONES**

### **Error: "cmake not found"**
- **Causa:** node-llama-cpp necesita compilar llama.cpp
- **Soluci√≥n:** ‚úÖ Ya a√±adido cmake al Dockerfile

### **Error: "git not found"**
- **Causa:** node-llama-cpp descarga source via git
- **Soluci√≥n:** ‚úÖ Ya a√±adido git al Dockerfile

### **GPU no detectada:**
- **Verificar:** `nvidia-smi` en host
- **Docker:** NVIDIA Container Toolkit instalado
- **Logs:** `npm run gpu:logs` para diagn√≥stico

### **Out of memory (VRAM):**
- **Reducir:** `GPU_LAYERS` de auto a n√∫mero menor
- **Aumentar:** `VRAM_PADDING` para estabilidad
- **Alternativa:** Usar modelo m√°s peque√±o

## üéâ **CONCLUSI√ìN**

node-llama-cpp v3.13.0 es muy inteligente:
- ‚úÖ **Auto-detecta** y configura GPU NVIDIA autom√°ticamente
- ‚úÖ **NO necesita** CUDA SDK instalado manualmente
- ‚úÖ **Compila optimizado** para tu hardware espec√≠fico
- ‚úÖ **Fallback robusto** a CPU si GPU no disponible
- ‚úÖ **Performance excelente** con configuraci√≥n m√≠nima

Tu configuraci√≥n en OASIS est√° bien optimizada! üöÄ